There's a strong distinction to be made between "wet code," interpreted by the
brain, and "dry code," interpreted by computers. Human-read media is wet code
whereas computer code and computer-readable files (to the extent a computer
deals meaningfully with them) are "dry code." Law is wet code, interpreted by
those on whom the law is imposed, and interpreted (often somewhat differently)
by law enforcers, but most authoritatively (and even more differently)
interpreted by judges. Human language is mostly wet code but to the extent
computer programs crudely translate from one language to another, keyword-ad
programs parse text to made an educated guess as to what ads a user will most
likely click, and so on, human language text can also be dry code. Traditional
contracts are wet code whereas smart contracts are mostly dry code. Secure
property titles and the domain name system are mostly dry code.  
  
Even "mostly dry code" often has surprisingly soggy portions. Smart contracts,
for example, can use dialogs to communicate their nature to the user and to
allow the user to at least input some parameters, make menu choices, and the
like. These are cognitive channels that translate between wet code and dry
code. Similarly, the human-readable name in the domain name system is wet code
and trademark law is wet code that applies to domain names. Phishing is an
attack against cognitive channels using ambiguous wet code, which is probably
why dry code programmers have a hard time coming to grips with it.  
  
The syntactic properties of wet and dry code are often very similar -- for
example, both human and computer language can usually be parsed by pushdown
automata into context-free grammars. But their semantic content is often
radically different, and the semantic content of wet code is often simply
unintelligible to a computer, for a variety of often mysterious reasons. If we
had good enough theories of human semantics we could program such theories
into the computer and the computer would then understand like a brain after
all. But we don't, in most areas, so we mostly can't program computers to
emulate humans. I believe these mysteries are primarily due to the highly
evolved nature of the brain contrasted to the recent and thus naive designs of
computers, especially with respect to computers' typical lack of good sensors
(they're mostly still far more sensory-deprived than Helen Keller, who had a
very informative sense of touch) and relative inability to learn from natural
and social environments. Their ability to aggregate disparate kinds of
information, for example through conceptual metaphor, is also relatively
undeveloped.  
  
As computers become "smarter" dry code is (very slowly) coming to do more that
was formerly only done by wet code. Once it successfully emulates wet code it
soon surpasses it in many respects; for example by now dry code can do simple
arithmetic billions of times faster than the typical human. As the idea of
smart contracts suggests, there are many fruitful analogies to be made from
wet code to dry code, but we must keep in mind the radically different
semantics, the strengths and weakness of each, and the need for each to
interact with the other to solve real problems.  
  
I don't think there is a "magic bullet" theory of artificial intelligence that
will uncover the semantic mysteries and give computers intelligence in one
fell swoop. I don't think that computers will mysteriously "wake up" one day
in some magic transition from zombie to qualia. (I basically agree with Daniel
Dennett in this respect). Instead, we will continue to chip away at
formalizing human intelligence, a few "bits" at a time, and will never reach a
"singularity" where all of a sudden we one day way wake up and realize
computers have surpassed us. Instead, there will be numerous "micro-runaways"
for particular narrow abilities that we learn how to teach computers to do,
such as the runaway over the last century or so in the superiority of
computers over humans in basic arithmetic. Computers and humans will continue
to co-evolve with computers making the faster progress but falling far short
of apocalyptic predictions of "Singularity," except to the extent that much of
civilization is already a rolling singularity. For example people can't
generally predict what's going to happen next in markets or which new startups
will succeed in the long run.

