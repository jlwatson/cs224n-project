What I was getting at in my posting is that *in exhibiting unwillingness to seriously consider the possibility that he's vastly overestimated his chances of building a Friendly AI* it appears that Eliezer is deviating sharply from leading a utilitarian lifestyle (relative to what one can expect from humans).

I was not trying to make a *general* statement about Eliezer's attainment of utilitarian goals relative to other humans. I think that there's a huge amount of uncertainty on this point to such an extent that it's meaningless to try to make a precise statement. 

The statement that I was driving at is a more narrow one.

I think that it would be better for Eliezer and for the world at large if Eliezer seriously considered the possibility that he's vastly overestimated his chances of building a Friendly AI. I strongly suspect that if he did this, his strategy for reducing existential risk would change for the better. If his current views turn out to be right, he can always return to them later on. I think that the expected benefits of him reevaluating his position far outweigh the expected costs.