&gt; Have your thought through all of the implications of a 1% discount rate?  ...almost the entire future of the universe will be determined by the values of those whose discount rates are lower than yours?

I don't know about thinking through *all* of the implications, but I have certainly thought through that one.  Which is one reason why I would advocate that any AI's that we build be hard-wired with a rather steep discount rate.  Entities with very low discount rates are extremely difficult to control through market incentives.  Murder is the only effective option, and the AI knows that, leading to a very unstable situation.

&gt; do you not have other areas where you don't know what you want?

Oh, I'm sure I do.  And I'm sure that what I want will change when I experience the Brave New World for myself.  That is why I advocate avoiding any situation in which I have to perfectly specify my fragile values correctly the first time - have to get it right because someone decided that the AI should make its own decisions about self-improvement and so we need to make sure its values are ultra-stable.

&gt; For example, what exactly is the nature of pleasure and pain? I don't want people to torture simulated humans, but what if they claim that the simulated humans have been subtly modified so that they only look like they're feeling pain, but aren't really? How can I tell if some computation is having pain or pleasure?

I certainly have some sympathy for people who find themselves in that kind of moral quandary.  Those kinds of problems just don't show up when your moral system requires no particular obligations to entities you have never met, with whom you cannot communicate, and with whom you have no direct or indirect agreements.

&gt; Have you already worked out all such problems, or at least know the principles by which you'll figure them out?

I presume you ask rhetorically, but as it happens, the answer is yes.  I at least know the principles.  My moral system is pretty simple - roughly a Humean rational self-interest, but as it would play out in a fictional society in which all actions are observed and all desires are known.  But that still presents me with moral quandaries - because in reality all desires *are not* known, and in order to act morally I need to know what other people want.

I find it odd that utilitarians seem less driven to find out what other people want than do egoists like myself.