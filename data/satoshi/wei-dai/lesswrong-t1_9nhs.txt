Every now and then, there are discussions or comments on LW where people talk about finding a "correct" morality, or where they argue that some particular morality is "mistaken". (Two recent examples: [[1]](http://lesswrong.com/lw/i1y/crossing_the_experiments_a_baby/9jl4) [[2]](http://lesswrong.com/lw/hox/effective_altruism_through_advertising/9bft)) Now I would understand that in an FAI context, where we want to find such a specification for an AI that it won't do something that all humans would find terrible, but that's generally not the context of those discussions. Outside such a context, it sounds like people were presuming the existence of an objective morality, but I thought that folks on LW rejected that. What's up with that?