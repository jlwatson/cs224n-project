So the head of BGI, famous for extremely ambitious &amp; expensive genetics projects which are a Chinese national flagship, is stepping down to work on AI because genetics is just too boring these days: http://www.nature.com/news/visionary-leader-of-china-s-genomics-powerhouse-steps-down-1.18059

I haven't been following estimates lately, but how much do people think it would cost in GPUs to approximate a human brain at this point given all the GPU performance leaps lately? I note that deep learning researchers seem to be training networks with up to [10b parameters using a 4 GPU](http://on-demand.gputechconf.com/gtc/2014/presentations/S4694-10-billion-parameter-neural-networks.pdf) setup costing, IIRC, &lt;$10k, and given the memory improvements [NVIDIA](http://blogs.nvidia.com/blog/2015/03/17/pascal/) &amp; AMD are working on, we can expect continued hardware improvements for at least another year or two.

(Schmidhuber's group is also now training networks with 100 layers using their new ['highway network'](http://arxiv.org/abs/1507.06228) design; I have to wonder if that has anything to do with Schmidhuber's new [NNAISENSE](https://nnaisense.com/) startup, beyond just Deepmind envy... EDIT: probably not if it was founded in [September 2014](http://www.moneyhouse.ch/en/u/v/nnaisense_sa_CH-501.3.019.165-6.htm) and the first highway network paper was pushed to arxiv in May 2015, unless Schmidhuber et al set it up to clear the way for commercializing their next innovation and highway networks is it.)