It has the same effect but at the cost of making the AI believe impossible things. This does not feel wise for an evolving superintelligence that can deduce that we influenced its prior - and why we did so. This might still be perfectly safe, or it might stumble on some meta-approach "if humans manipulate your priors in this way, they are making you a less effective agent". And since it still has preference where X=0 is better than X=1, it may act to change its own prior.

There is another issue, for situations where we might want to use indifference for less terminal things than its own destruction. One idea I had was to make an Oracle AI indifferent to the consequences of its own answers, by pipping its answers through a quantum process that has an infinitesimal chance of erasing them before we see them. If we do this through probability manipulation rather than utility manipulation, then afterwards it will know that an impossible event just happened, probably not a healthy thing for an AI.