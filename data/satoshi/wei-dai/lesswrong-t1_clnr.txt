Here is a novel argument you may or may not have heard:  We live in the best of all probable worlds due to simulation anthropics.  Future FAI civs spend a significant amount of their resources to resimulate and resurrect past humanity - winning the sim race by a landslide (as UFAI is not strongly motivated to sim us in large numbers).  As a result of this anthropic selection force, we find ourselves in a universe that is very lucky - it is far more likely to lead to FAI than you would otherwise think.

The best standard argument is this: the brain is a universal learning machine - the same general architecture that will necessarily form the basis for any practical AGI.  In addition the brain is already near optimal in terms of what can be done for 10 watts with any irreversible learning machine (this is relatively easy to show from wiring energy analysis).  Thus any practical AGI is going to be roughly brain like, similar to baby emulations.  All of the techniques used to raise humans safely can thus be used to raise AGI safely.  LW/MIRI historically reject this argument based - as far as I can tell - on a handwavey notion of 'anthropomorphic bias', which has no technical foundation.

I've presented the above argument about four years ago, but I never bothered to spend the time backing it up in excruciating formal detail.  Until [more recently](http://lesswrong.com/lw/md2/the_brain_as_a_universal_learning_machine/).  The last 5 years of progress in AI strongly supports this anthropomorphic AGI viewpoint.