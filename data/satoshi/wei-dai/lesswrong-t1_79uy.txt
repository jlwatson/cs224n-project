*Question:* Why don't people talk about Ems / Uploads as just as disastrous as uncontrolled AGI?  Has there been work done or discussion about the friendliness of Ems / Uploads?

*Details:* Robin Hanson seems to describe the Em age like a new industrial revolution.  Eliezer seems to, well, he seems wary of them but doesn't seem to treat them like an existential threat.  Though Nick Bostrom sees them as an existential threat.  A lot of people on Lesswrong seem to talk of it as the next great journey for humanity, and not just a different name for uFAI.  For my part, I can't imagine uploads ending up good.  I literally can't imagine it.  Every scenario I've tried to imagine ends up with a bad end.  

As soon as the first upload is successful then patient zero will realize he's got unimaginable (brain)power, will start talking in ALL CAPS, and go FOOM on the world, bad end.  For the sake of argument, lets say we get lucky and first upload is incredibly nice, and just wants to help people.  Eventually the second, or the third, or the twenty fifth upload decides to FOOM over everybody.  It's still bad end.  We need to have some way to restrain Ems from FOOM-ing, and we need to figure it out before we start uploading.  Okay, lets pretend we could even invent a restraint that works against a determined transhiman who is unimaginably more intelligent than us...

Maybe we'll get as far as, say, Hanson's Em society.  Ems make copies of themselves tailored to situations to complete work.  Some of these copies will choose to / be able to replicate more than others; these copies will inherit this propensity to replicate; eventually, processor-time / RAM-time / hard-disk space will become scarce and things won't be able to copy as well and will have to fight to not have their processes terminated.  Welp... that sounds like the 3 ingredients required to invoke the evolution fairy.  Except instead of it being the Darwinian evolution we're used to, this new breed will employ a terrifying mix of uFIA self-modification and Lamarckian super-evolution.  Bad end.  Okay, but lets say we find some way to stop THAT...

What about other threats? Ems can still talk to one another and *convince* one another of things.  How do we know they won't all be hijacked by meme-viruses, and transformed Agent Smith style?  That's a bad end.  Or hell, how do we know they won't be hijacked by virus-viruses?  Bad end there too.  Or one of the trillions of Ems could build a uFAI and it goes FOOM into a Bad End.  Or... The potential for Bad Ends is enormous and you only need one for the end of humanity.

It's not like flesh-based humans can monitor the system.  Once ems are in the 1,000,000x era, they'll be effectively decoupled from humanity.  A revolution could start at 10pm after the evening shift goes home, and by the time the morning shift gets in, it's been 1,000 years in Em subjective time.  Hell, in the time it takes to swing an axe and cut the network/power cable, they've had about a month to manage their migration and dissemination to every electronic device in the world.  Any regulation has to be built inside the Em system and, as mentioned before, it has to be built before we make the first successful upload.

Maybe we can build an invincible regulator or regulation institution to control it all.  But we can't let it self-replicate or we'll be right back at the evolution problem again.  And we can't let it be modified by the outside world or it'll be the hijacking problem again.  And we can't let it self-modify, or it'll evolve in ways we can't predict (and we've already established that it'll be outside of everything else's control).  So now we have an invulnerable regulator/regulation system that needs to control a world of *trillions*.  And once our Ems start living in 1,000,000x space, it needs to keep order for literally *millions of years* without ever making a mistake once.  So we need to design a system perfect enough to never make a single error while handling trillions of agents for millions of years?

That strikes me as a problem that's just as hard as FAI.  There seems like no way to solve it that doesn't involve a friendly AGI controlling the upload world.

Can anyone explain to me why Ems are looked at as a competing technology to FAI instead an existential risk with probability of 1.0?