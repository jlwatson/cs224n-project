&gt; We humans seem at best just barely smart enough to build a superintelligent UFAI. Wouldn't it be surprising that the intelligence threshold for building UFAI and FAI turn out to be the same?

I think people who would contest the direction of this post (probably Eli and Nesov) would point out that if humanity is over the intelligence threshold for UFAI, the economic-political-psychological forces would drive it to be built in a timeframe of few decades. Anything that does not address this directly will destroy the future. Building smarter humans is likely not fast enough (plus who says smarter humans will not be driven by the same forces to build UFAI?). 