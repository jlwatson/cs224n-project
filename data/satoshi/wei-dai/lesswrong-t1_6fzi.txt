As far as I can tell, some of the most recent conversations to have the most uncvil remarks are conversations involving whether AI risk is a serious problem and if so what should be done about it. The [thread on Luke's discussion with Pei Wang](http://lesswrong.com/r/discussion/lw/bxr/muehlhauserwang_dialogue/) seems to be the most recent example. This also appears to be more common in threads that discuss mainstream attitudes about AI risk and where they disagrees with common LW opinion. Given that, I'm becoming worried that AI risk estimates may be becoming a tribalized belief category. Should we worry that AI risk is becoming or has become a mindkiller? 