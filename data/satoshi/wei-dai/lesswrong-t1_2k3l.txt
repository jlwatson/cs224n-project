&gt;What evidence do you have of this? 

Eliezer was not willing to engage with my [estimate here](http://lesswrong.com/lw/2lr/the_importance_of_selfdoubt/2h9y?c=1). See [his response](http://lesswrong.com/lw/2lr/the_importance_of_selfdoubt/2ha0?c=1). For the reasons that I point out [here](http://lesswrong.com/lw/2lr/the_importance_of_selfdoubt/2k3a?c=1), I think that my estimate is well grounded.

Eliezer's apparent lack of willingness to engage with me on this point does not immediately imply that he's unwilling to seriously consider the possibility that I raise. But I do see it as strongly suggestive.

As I said [in response to ThomBlake](http://lesswrong.com/lw/2lr/the_importance_of_selfdoubt/2k3h?c=1), I would be happy to pointed to any of Eliezer's writings which support the idea that Eliezer has given serious consideration to the two points that I raised to explain my estimate.

**Edit:** I'll also add that given the amount of evidence that I see against the proposition that Eliezer will build a Friendly AI, I have difficulty imagining how he could be persisting in holding his beliefs *without* having failed to give serious consideration to the possibility that he might be totally wrong. It seems very likely to me that if he had explored this line of thought, he would have a very different world view than he does at present.