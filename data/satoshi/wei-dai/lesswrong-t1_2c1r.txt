So... the part I found potentially convincing was that if you ran off a logical view of the world instead of a Solomonoff view (i.e., beliefs represented in e.g. higher-order logic instead of Turing machines) and lived in a hypercomputable world then it might be possible to make better *decisions*, although not better *predictions of sensory experience*, in some cases where you can infer by reasoning symbolically that EU(A) &gt; EU(B), presuming that your utility function is itself reasoning over models of the world represented symbolically.  On the other hand, cousin_it's original example still looks wrong.