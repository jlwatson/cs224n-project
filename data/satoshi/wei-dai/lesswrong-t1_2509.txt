... okay, this question allowed me to make a bit of progress. Taking as a starting point the setting of [this comment](http://lesswrong.com/lw/2bt/udt_agents_as_deontologists/24xh) (that we are estimating the probability of  (A~B =&gt; X~Y) being true, where A and X are respectively agent's and environment's programs, B and Y programs representing agent's strategy and outcome for environment), and the observations made [here](http://lesswrong.com/lw/2bt/udt_agents_as_deontologists/24zr) and [here](http://lesswrong.com/lw/2bt/udt_agents_as_deontologists/2501), we get a scheme for local decision-making.

Instead of trying to decide the whole strategy, we can just decide the local action. Then, the agent program, and "input" consisting of observations and memories, together make up the description of where the agent is in the environment, and thus where its control will be applied. The action that the agent considers can then be local, just something the agent does at this very moment, and the alternatives for this action are alternative statements about the agent: thus, instead of considering a statement A~B for agent's program A and various whole strategies B, we consider just predicates like action1(A) and action2(A) which assert A to choose action 1 or action 2 in this particular situation, and which don't assert anything else about its behavior in other situations or on other counterfactuals. Taking into account other actions that the agent might have to make in the past or in the future happens automatically, because the agent works with complete description of environment, even if under severe logical uncertainty. Thus, decision-making happens "one bit at a time", and the agent's strategy mostly exists in the environment, not under in any way *direct* control by the agent, but still controlled in the same sense everything in the environment is.

Thus, in the simplest case of a binary local decision, mathematical intuition would only take as explicit argument a single bit, which indicates what assertion is being made about \[agent's program together with memory and observations\], and that is all. No maps, no untyped strategies. 

This solution was unavailable to me when I thought about explicit control, because the agent has to coordinate with itself, rely on what it can in fact decide in other situations and not what it should optimally decide, but it's a natural step in the setting of ambient control, because the incorrect counterfactuals are completely banished out of consideration, and environment describes what the agent will actually do on other occasions.

Going back to the post [explicit optimization of global strategy](http://lesswrong.com/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/), the agent doesn't need to figure out the global strategy! Each of the agent copies is allowed to make the decision locally, while observing the other copy as part of the environment (in fact, it's the same problem as "general coordination problem" I described on the DT list, back when I was clueless about this approach).