When answer is robustly unattainable, it's pointless to speculate what it might be, you can only bet or build conditional plans. If "values" are "simple", but you don't know that, your state of knowledge about the "values" remains non-simple, and that is what you impart to AI. What does it matter which of these things, the confused state of knowledge or the correct answer to our confusion, do we call "values"?