&gt;What does this robot "actually want", given that the world is not really a 2D grid of cells that have intrinsic color?

Who cares about the question what the robot "actually wants"? Certainly not the robot. *Humans* care about the question what *they* "actually want", but that's because they have additional structure that this robot lacks. But with humans, you're not limited to just looking at what they do on auto-pilot; instead, you can *just ask* the aforementioned structure when you run into problems like this. For example, if you asked me what I really wanted under some weird ontology change, I could say, "I have some guesses, but I don't really know; I would like to defer to a smarter version of me". That's how I understand preference extrapolation: not as something that looks at what your behavior suggests that you're trying to do and then does it better, but as something that poses the question of what you want to some system you'd like to answer the question for you. 

It looks to me like there's a mistaken tendency among many people here, including some very smart people, to say that I'd be irrational to let my stated preferences deviate from my revealed preferences; that just because I seem to be trying to do something (in some sense like: when my behavior isn't being controlled much by the output of moral philosophy, I can be modeled as a relatively good fit to a robot with some particular utility function), that's a *reason* for me to do it even if I decide that I don't *want* to. But rational utility maximizers get to be indifferent to whatever the heck they want, including [their own preferences](http://lesswrong.com/lw/5sk/inferring_our_desires/48pz), so it's hard for me to see why the underdeterminedness of the true preferences of robots like this should bother me at all.

Insert standard low confidence about me posting claims on complicated topics that others seem to disagree with.