&gt; I think you're invoking TDT-style reasoning here, before the agent has self-modified into TDT.

I already said that agents which start out as *pure* CDT won't modify into pure TDTs - they'll only cooperate if someone gets a peek at their source code after they self-modified.  However, humans, at least, are not pure CDT agents - they feel at least the *impulse* to one-box on Newcomb's Problem if you raise the stakes high enough.

This has nothing to do with evolutionary contexts of honor and cooperation and defection and temptation, and everything to do with our evolved instincts governing *abstract logic and causality*, which is what governs *what sort of source code you think has what sort of effect*.  Even *unreasonably pure* CDT agents recognize that if they modify their source code at 7am, they should modify to play TDT against any agent that has looked at their source code *after* 7am.  To humans, who are not *pure* CDT agents, the idea that you should play essentially the same way if Omega glimpsed your source code at exactly 6:59am, seems like *common sense* given the intuitions we have about logic and causality and elegance and winning.  If you're going to all the trouble to invent TDT anyway, it seems like a *waste of effort* to two-box against Omega if he perfectly saw your source code 5 seconds before you self-modified.  (These being the kind of ineffable meta-decision considerations that we both agree are important, but which are hard to formalize.)

&gt; Besides, I'm assuming a world where agents can't know or guess each others' source codes. 

You are *guessing their source code* every time you *argue that they'll choose D*.  If I can't make you see this as an instance of "guessing the other agent's source code" then indeed you will not see the large correlations at the start point, and if the agents start out highly uncorrelated then the *rare* TDT agents will *choose the correct maximizing action, D*.  They will be rare because, by assumption in this case, most agents end up choosing to cooperate or defect for *all sorts of different reasons*, rather than by following *highly regular lines of logic in nearly all cases* - let alone the *same* line of logic that kept on predictably ending up at D.

There's a wide variety of cases where philosophers go astray by failing to recognize an instance of an everyday concept as an abstract concept.  For example, they say in one breath that "God is unfalsifiable", and in the next breath talk about how God spoke to them in their heart, because they don't recognize "God spoke to me in my heart" as an instance of "God allegedly made something observable happen".  Philosophers talk about qualia being epiphenomenal in one breath, and then in the next speak of how they know themselves to be conscious, because they don't recognize this self-observation as an instance of "something making something else happen" aka "cause and effect".  The only things recognized as matching the formal-sounding phrase "cause and effect" are big formal things officially labeled "causal", not just stuff that makes other stuff happen.

In the same sense, you have this idea about modeling other agents as this big official affair that requires poring over their source code with a magnifying glass and then furthermore verifying that they can't change it while you aren't looking.

You need to recognize *the very thought processes you are carrying out right now* in arguing that *just about anyone will choose D* as an instance of *guessing the outputs of the other agents' source codes* and moreover guessing that *most such codes and outputs are massively logically correlated*.

This is witnessed by the fact that if we did get to see some interstellar transactions, and you saw that the first three transactions were (C, C), you would say, "Wow, guess Eliezer was right" and expect the next one to be (C, C) as well.  (And of course if I witnessed three cases of (D, D) I would say "Guess I was wrong.")  Even though the initial conditions are not *physically correlated*, we expect a correlation.  What is this correlation, then?  It is a logical correlation.  We expect different species to end up following similar lines of reasoning, that is, performing similar computations, like factorizing 123,456 in spacelike separated galaxies.