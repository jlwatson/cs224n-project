&gt; I'm not truly impressed with GiveWell's general optimization since they never made a good case that malaria was connected to astronomical benefits or, indeed, seem to have realized that such a case is necessary for effective altruism.

Well, but I'm not sure MIRI can be said to have "made a good case" that its *own* work is well-connected to astronomical benefits, either. Presumably the argument for that looks something like the [FAI Research as Effective Altruism](http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/) argument, but that argument hasn't been made in much detail, with the key assumptions clearly identified and argued for with clarity and solid evidential backing. E.g.:

* I'm not aware of a thorough, empirical (written) investigation of [whether elites will handle AI just fine](http://lesswrong.com/lw/hlc/will_the_worlds_elites_navigate_the_creation_of/).
* Beckstead's [2013 thesis](https://sites.google.com/site/nbeckstead/research/Beckstead%2C%20Nick--On%20the%20Overwhelming%20Importance%20of%20Shaping%20the%20Far%20Future.pdf?attredirects=0&amp;d=1) is the first document I'm aware of that clearly lays out all the assumptions baked into the argument for the overwhelming importance of the far future.
* My 2013 post [When Will AI Be Created?](http://intelligence.org/2013/05/15/when-will-ai-be-created/) is (I think) the best available piece for capturing the enormous difficulties of predicting AI — with reference to lots of relevant empirical data — while also (barely) making the case for assigning a good chunk of one's probability mass to getting AI this century. But it's still pretty inadequate, and the part making the case for the plausibility of AI this century could be substantially improved if more time was invested. (Compare to [Bostrom 1998](http://www.nickbostrom.com/superintelligence.html), which I find inadequate. I also think it will now look naively timelines-optimistic to most observers.)

Moreover, it's not that Givewell (well, Holden) hasn't "realized" that recommended altruistic interventions (e.g. bednets) need to be connected via argument to astronomical benefits. Rather, Holden has been aware of astronomical waste arguments for a long time, and has reasons for rejecting them. He also discussed astronomical waste arguments many times with Beckstead while Beckstead was writing his dissertation. Unfortunately, Holden has struggled to clearly express his reasons for rejecting astronomical waste arguments. He tried to explain his reasons to me in person once but I couldn't make sense of what he was saying. He also tried to explain his point in the last three paragraphs of [this comment](http://lesswrong.com/lw/gzq/bayesian_adjustment_does_not_defeat_existential/8nto), but I, at least, still don't understand quite what he's saying. Explaining is hard.

Also, Holden has spent a lot of time working up to an explanation of why he (currently) thinks that (1) "generic good work"  (which may indirectly produce astronomical benefits via ripple effects) has higher expected value than (2) narrow interventions aimed more *directly* at astronomical benefit. His two latest posts in this thread are [Flow-through effects](http://blog.givewell.org/2013/05/15/flow-through-effects/) and [Possible global catastrophic risks](http://blog.givewell.org/2013/05/23/possible-global-catastrophic-risks/), and he has [promised](http://blog.givewell.org/2013/05/23/possible-global-catastrophic-risks/) that "a future post will discuss how I think about the overall contribution of economic/technological development to our odds of having a very bright, as opposed to very problematic, future."

And all this during the early years in which GiveWell mostly hasn't been investigating trickier issues like how different interventions connect to potential astronomical benefits, because GiveWell (wisely, I think) decided to [start under the streetlight](http://lesswrong.com/lw/hsd/start_under_the_streetlight_then_push_into_the/).