How do you play "cooperate iff (the opponent cooperates iff I cooperate)" in a GLT?  Is the *programmer* supposed to be modeling the opponent AI in sufficient resolution to guess how much the *opponent AI* knows about the programmer's decision, and how many other possible programmers that the AI is modeling are likely to correlate with it?  Does S compute the programmer's decision using S's knowledge or only the programmer's knowledge?  Does S compute the opponent inaccurately as if it were modeling only the programmer, or accurately as if it were modeling both the programmer and S?

I suppose that a strict CDT could replace itself with a GLT, if that GLT can take into account all info where the opponent AI gets a glimpse at the GLT *after* it's written.  Then the GLT behaves just like the code I specified before on e.g. Newcomb's Problem - one-box if Omega glimpses the GLT or gets evidence about it after the GLT was written, two-box if Omega perfectly knows your code 5 seconds before the GLT gets written.