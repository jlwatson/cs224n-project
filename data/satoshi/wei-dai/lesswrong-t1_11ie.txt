Even apart from apparent mind-killing properties of this proposal, I don't think it's reasonable. First, it's unnecessary: if you expect the probability of a positive intelligence explosion to go up even a little bit as a result of your donation, the crazy positive utility of the outcome compensates for the donation. If you don't think the donation affects the outcome, don't donate. Second, implementation of some compensation mechanism is an ad-hoc rule that isn't necessarily possible to attach to the AI's goals in a harmless way, so you can't promise that it will be done. Also, if something of the kind is really a good idea, FAI should be able to implement it regardless of what you promise now, without tweaking of its goals.