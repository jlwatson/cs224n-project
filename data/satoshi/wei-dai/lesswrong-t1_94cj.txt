&gt;we don't yet know what principles allow us to say that P!=NP is more likely to be true than false

We have coherent answers at least. See e.g. [here](http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_70.pdf) for a formalism (and similarly the much older stuff by Gaifman, which didn't get into priors). MIRI is working much more directly on this problem as well. Can you think of concrete open questions in that space? Basically we are just trying to develop the theory, but having simple concrete problems would surely be good. (I have a bucket of standard toy problems to resolve, and don't have a good approach that handle all of them, but it's pretty easy to hack together a solution to them so they don't really count as open problems.)

I agree that AI progress is probably socially costly (highly positive for currently living folks, modestly negative for the average far future person). I think work with a theoretical bias is more likely to be helpful, and I don't think it is very bad on net. Moreover, as long as safety-concerned folks are responsible for a very small share of all of the good AI work, the reputation impacts of doing good work seem very large compared to the social benefits or costs.

We don't know that probabilities are the right way to handle logical uncertainty, nor that our problem statements are correct. I think that the kind of probabilistic reflection we are working on is fairly natural though.

I agree with both you and Nick that the strategic questions are very important, probably more important than the math. I don't think that is inconsistent with getting the mathematical research program up and going. I would guess that all told the math will help on the strategy front via building the general credibility of AI safety concern (by 1. making it clear that there are concrete policy-relevant questions here, and 2. building status and credibility for safety-concerned communities and individuals), but even neglecting that I think it would still be worth it.