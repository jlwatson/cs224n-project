In the "Learning from examples" case, Arthur looks a lot like AIXI with a time horizon of 1 (i.e., one that acts to maximize just the expected next reward), and I don't understand why you say "But unlike AIXI, Arthur will make no effort to manipulate these judgments." For example, it seems like Arthur could learn a model in which approval\[*T*\]\(*a*\) = 1 if *a* is an action which results in taking over the approval input terminal and giving itself maximum approval.