Is there a difference between a value loading agent that changes its utility function based on evidence, and a UDT agent with a fixed utility function over all possible universes, which represents different preferences for different universes (e.g., it prefers more cakes in the universe where the programmer says "I want cake" and more deaths in the universe where the programmer says "I want death")?