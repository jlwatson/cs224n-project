"AI Nanny" does seem even harder than FAI (the [usual arguments](http://lesswrong.com/lw/716/complex_value_systems_are_required_to_realize/) apply to it with similar strength, but it is additionally asked for a specific wish), and compared to no-worries-AGI this idea has better immunity to arguments about the danger of its development. It's a sufficiently amorphous proposal to shroud many AGI projects without essentially changing anything about them, including project members' understanding of AI risk. So on the net, this looks to me like a potentially negative development.