&gt; I'm familiar with this move. But you make it before failing too

Sure, you try, sometimes you lose, sometimes you *win.*  On anti-heroic epistemology (non-virtuous to attempt to discriminate within an outside view) there shouldn't be *any* impossible successes by anyone you know personally after you met them.  They should only happen to other people selected post-facto by the media, or to people who you met because of their previous success.

&gt; I don't buy the framing. The update would be mainly about you and the problem in question, not the applicability of statistics to reality.

We disagree about how to use statistics in order to get really actually correct answers.  Having such a low estimate of my rationality that [you think that I know what correct statistics are, and am refusing to use them](https://www.facebook.com/yudkowsky/posts/10151777408634228), is not good news from an Aumann perspective and fails the ideological Turing Test.  In any case, surely if my predictions are correct you should update your belief about good frameworks (see the reasoning used in the Pascal's Muggle post) - to do otherwise and go on insisting that your framework was nonetheless correct would be *oblivious*.

&gt; Two developments in AI as big as Pearl's causal networks (as judged by Norvig types)

...should not have been disclosed to the general world, since proof well short of this should suffice for sufficient funding (Bayes nets were *huge*), though they might be disclosed to some particular Norvig type on a trusted oversight committee if there were some kind of reason for the risk.  Major breakthroughs on the F side of FAI are not likely to be regarded as being as exciting as AGI-useful work like Bayes nets, though they may be equally mathematically impressive or mathematically difficult.  Is there some kind of validation which you think MIRI should not be able to achieve on non-heroic premises, such that the results *should* be disclosed to the general world?

EDIT:  Reading through the rest of the comment more carefully, I'm not sure we estimate the same order of magnitude of work for what it takes to build FAI under mildly good background settings of hidden variables.  The reason why I don't think the mainstream can build FAI isn't that FAI is intrinsically huge a la the Cyc hypothesis.  The mainstream is pretty good at building huge straightforward things.  I just expect them to run afoul of one of the many instakill gotchas because they're one or two orders of magnitude underneath the finite level of caring required.

EDIT 2:  Also, is there a level short of 2 gigantic breakthroughs which causes you to question non-heroic epistemology?  The condition is sufficient, but is it necessary?  Do you start to doubt the framework after one giant breakthrough (leaving aside the translation question for now)?  If not, what probability would you assign to that, on your framework?  Standard Bayesian Judo applies - if you would, as I see it, play the role of the skeptic, then you must either be overly-credulous-for-the-role that we can do heroic things like *one* giant breakthrough, or else give up your skepticism at an earlier signal than the second.  For you cannot say that something is strongly prohibited on your model and yet also refuse to update much if it happens, and this applies to every event which might lie along the way.  (Evenhanded application:  'Tis why I updated on Quixey instead of saying "Ah, but blah"; Quixey getting this far just wasn't supposed to happen on my previous background theory, and shouldn't have happened even if Vassar had praised ten people to me instead of two.)