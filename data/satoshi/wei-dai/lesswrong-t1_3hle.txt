I think we should figure out what our true utility function is, and *then*, if we have reason to be worried about bugs/errors in the AI, determine how to prevent or mitigate against such errors, which may include programming an utility function into the AI that's not the true utility function, but an approximation that is less vulnerable to errors. I don't see how we can do that without knowing the true utility function, or the likelihood of various kinds of errors. It seems like you're jumping way too far ahead.