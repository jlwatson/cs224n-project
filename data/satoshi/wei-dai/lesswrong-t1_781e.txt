&gt; Suppose we define a generalized version of Solomonoff Induction based on some second-order logic. The truth predicate for this logic canâ€™t be defined within the logic and therefore a device that can decide the truth value of arbitrary statements in this logical has no finite description within this logic. If an alien claimed to have such a device, this generalized Solomonoff induction would assign the hypothesis that they're telling the truth zero probability, whereas we would assign it some small but positive probability

Actually, what Tarski seems to show is that for *any* language for describing *any* set of universes, there just *is* no language representable inside those universes for *representing* arbitrary statements, with truth values, about "everything" including the language and the statements in it.  If you try to invent such a language, it will end up inconsistent - not at the point where it tries to correctly assign truth, but at the point where it can *represent* truth, due to analogues of "This statement is false."  It isn't needful to assign 0 or 1, in particular, to this statement; the moment you represent it, you can prove an inconsistency.  But is this really proper to blame on Solomonoff induction?