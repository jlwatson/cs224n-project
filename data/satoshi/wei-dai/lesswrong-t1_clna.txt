You're missing my point, which is that proposing you've got "an AI" (with no dissolved understanding of how the thing actually works *underneath* what you'd get from a Greg Egan novel) which "simulates" possible "worlds" is already engaging in several layers of magical thinking, and you shouldn't be surprised to draw silly conclusions from magical thinking.