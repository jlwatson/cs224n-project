We certainly *could* integrate known strategic arguments into a quantitative framework [like this](http://lesswrong.com/lw/bjl/ai_risk_opportunity_strategic_analysis_via/), but [I'm worried](http://lesswrong.com/lw/bjl/ai_risk_opportunity_strategic_analysis_via/69l1) that, for example, "putting so *many* made-up probabilities into a probability tree like this is [not actually that helpful](http://lesswrong.com/lw/sg/when_not_to_use_probabilities/)."

I think for now both SI and FHI are still in the qualitative stage that normally precedes quantitative analysis. Big projects like Nick's monograph and SI's AI risk wiki will indeed constitute "rapid progress" in strategic reasoning, but it will be rapid progress *toward* more quantitative analyses, not rapid progress within a quantitative framework that we have already built. 

Of course, some of the work on strategic *sub-problems* is *already* at the quantitative/formal stage, so quantitative/formal progress can be made on them immediately if SI/FHI can raise the resources to find and hire the right people to work on them. Two examples: (1) What do reasonable economic models of past jumps in optimization power imply about what would happen once we get self-improving AGI? (2) If we add lots more AI-related performance curve data to Nagy's [Performance Curve Database](http://pcdb.santafe.edu/) and use [his improved tech forecasting methods](http://tuvalu.santafe.edu/~bn/workingpapers/NagyFarmerTrancikBui.pdf), what does it all imply about AI and WBE timelines?