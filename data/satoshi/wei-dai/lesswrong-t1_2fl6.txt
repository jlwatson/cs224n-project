[This post](http://lesswrong.com/lw/66/rationality_common_interest_of_many_causes/) seems to be Eliezer's own counter/qualification to [Purchase Fuzzies and Utilons Separately](http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/). It seems very relevant here, and I'm surprised nobody has brought it up yet. Here's a quote:

&gt;If we're operating under the assumption that everyone by default is an altruistic akrasic (someone who wishes they could choose to do more) - or at least, that most potential supporters of interest fit this description - then fighting it out over which cause is the best to support, may have the effect of decreasing the overall supply of altruism.
&gt;
&gt;"But," you say, "dollars are fungible; a dollar you use for one thing indeed cannot be used for anything else!"  To which I reply:  But human beings really aren't expected utility maximizers, as cognitive systems.  Dollars come out of different mental accounts, cost different amounts of willpower (the true limiting resource) under different circumstances, people want to spread their donations around as an act of mental accounting to minimize the regret if a single cause fails, and telling someone about an additional cause may increase the total amount they're willing to help.