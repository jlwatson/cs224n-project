For instance, [MIRI's 2013 strategy](http://intelligence.org/2013/04/13/miris-strategy-for-2013/) mostly involves making math progress and trying to get mathematicians in academia interested in these kinds of problems, which is a different approach from the "small FAI team" one that you focus on in your post. As another kind of approach, the considerations outlined in [AGI Impact Experts and Friendly AI Experts](http://intelligence.org/2013/05/01/agi-impacts-experts-and-friendly-ai-experts/) would suggest a program of generally training people with an expertise in AI safety questions, in order to have safety experts involved in many different AI projects. There have also been [various proposals](http://intelligence.org/files/ResponsesAGIRisk.pdf) about eventually pushing for regulation of AI, though MIRI's comparative advantage is probably more on the side of technical research.