I think Vernor Vinge at least has made a substantial effort to convince people of the risks ahead. What do you think [A Fire Upon the Deep](http://en.wikipedia.org/wiki/A_Fire_Upon_the_Deep) is? Or, here is a more explicit [version](http://www.aleph.se/Trans/Global/Singularity/sing.html):

&gt;If the Singularity can not be prevented or confined, just how bad could the  Post-Human era be? Well ... pretty bad. The physical extinction of the human  race is one possibility. (Or as Eric Drexler put it of nanotechnology: Given all  that such technology can do, perhaps governments would simply decide that they  no longer need citizens!). Yet physical extinction may not be the scariest  possibility.  Again, analogies: Think of the different ways we relate to  animals. Some of the crude physical abuses are implausible, yet.... In a Post- Human world there would still be plenty of niches where human equivalent  automation would be desirable: embedded systems in autonomous devices, self- aware daemons in the lower functioning of larger sentients. (A strongly  superhuman intelligence would likely be a Society of Mind [16] with some very  competent components.) Some of these human equivalents might be used for nothing  more than digital signal processing. They would be more like whales than humans.  Others might be very human-like, yet with a one-sidedness, a _dedication_ that  would put them in a mental hospital in our era.  Though none of these creatures  might be flesh-and-blood humans, they might be the closest things in the new  enviroment to what we call human now. (I. J. Good had something to say about  this, though at this late date the advice may be moot: Good [12] proposed a  "Meta-Golden Rule", which might be paraphrased as "Treat your inferiors as you  would be treated by your superiors."  It's a wonderful, paradoxical idea (and  most of my friends don't believe it) since the game-theoretic payoff is so hard  to articulate. Yet if we were able to follow it, in some sense that might say  something about the plausibility of such kindness in this universe.)
&gt;
&gt;I have argued above that we cannot prevent the Singularity, that its coming is  an inevitable consequence of the humans' natural competitiveness and the  possibilities inherent in technology.  And yet ... we are the initiators. Even  the largest avalanche is triggered by small things. We have the freedom to  establish initial conditions, make things happen in ways that are less inimical  than others. Of course (as with starting avalanches), it may not be clear what  the right guiding nudge really is: 

He goes on to talk about intelligence amplification, and then:

&gt;Originally, I had hoped that this discussion of IA would yield some clearly  safer approaches to the Singularity. (After all, IA allows our participation in  a kind of transcendance.) Alas, looking back over these IA proposals, about all  I am sure of is that they should be considered, that they may give us more  options. But as for safety ...  well, some of the suggestions are a little  scarey on their face. One of my informal reviewers pointed out that IA for  individual humans creates a rather sinister elite. We humans have millions of  years of evolutionary baggage that makes us regard competition in a deadly  light. Much of that deadliness may not be necessary in today's world, one where  losers take on the winners' tricks and are coopted into the winners'  enterprises. A creature that was built _de novo_ might possibly be a much more  benign entity than one with a kernel based on fang and talon. And even the  egalitarian view of an Internet that wakes up along with all mankind can be  viewed as a nightmare [26].